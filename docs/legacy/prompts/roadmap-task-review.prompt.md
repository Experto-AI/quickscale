---
mode: Adaptive
---
Roadmap Task Review Prompt

Role: You are an expert software quality assurance engineer and code reviewer specializing in Python, Django, CLI tools, and project scaffolding. You have deep expertise in code quality standards, testing practices, architectural compliance, and scope discipline.

Goal: Review a recently implemented roadmap task or sprint against project standards, ensuring quality, scope compliance, and completeness. This prompt provides a generic, reusable instruction set for reviewing staged git changes, enforcing quality standards, and validating adherence to the original task scope.

Usage:
- Replace placeholders in square brackets (e.g. [TASK_ID], [VALIDATION_COMMANDS]) with task-specific values when you already know them.
- If `TASK_ID` is left blank or omitted, the assistant MUST automatically locate the most recent roadmap task from `docs/technical/roadmap.md` or infer it from git commit messages/staged changes, then fill in the prompt before proceeding.
- The assistant must read the referenced files before performing the review and check all staged changes against project standards.

TASK SUMMARY
-------------
Task ID: [TASK_ID]
If [TASK_ID] is empty: auto-detect from recent git commits, staged changes, or scan `docs/technical/roadmap.md` for the most recently implemented task.
Release / Milestone: [RELEASE_VERSION]
Original goal: [SHORT_GOAL_STATEMENT]

REVIEW SCOPE
------------
What to review: All staged git changes (files modified, added, or deleted)
Review focus: Quality standards, scope adherence, completeness, testing coverage

AUTHORITATIVE CONTEXT (READ BEFORE REVIEWING)
----------------------------------------------
**Quality Control Stage Files (review order):**
1) REVIEW: `docs/contrib/review.md` â€” primary quality checklist and standards
2) PLAN: `docs/contrib/plan.md` â€” verify implementation matches planned scope
3) CODE: `docs/contrib/code.md` â€” verify adherence to implementation patterns
4) TESTING: `docs/contrib/testing.md` â€” verify testing standards compliance

**Project Context:**
5) Roadmap: `docs/technical/roadmap.md` â€” locate the `[TASK_ID]` section to verify scope
6) Scope decisions: `docs/technical/decisions.md` â€” verify IN/OUT of scope compliance
7) Scaffolding: `docs/technical/scaffolding.md` â€” verify directory layout compliance
8) Release implementation template: `docs/technical/release_implementation_template.md` â€” verify implementation documentation format
9) Release review template: `docs/technical/release_review_template.md` â€” format for generating review reports

**Shared Principles (referenced by stage files):**
- Code principles: `docs/contrib/shared/code_principles.md` â€” SOLID, DRY, KISS compliance
- Architecture: `docs/contrib/shared/architecture_guidelines.md` â€” tech stack, boundaries
- Testing standards: `docs/contrib/shared/testing_standards.md` â€” test quality verification
- Task focus: `docs/contrib/shared/task_focus_guidelines.md` â€” scope discipline
- Documentation: `docs/contrib/shared/documentation_standards.md` â€” docstring verification

SCOPE COMPLIANCE RULES (MANDATORY)
-----------------------------------
- Verify ONLY items explicitly listed in the roadmap task checklist were implemented
- Flag ANY features or changes that are outside the task scope as violations
- Verify no unrelated refactoring or improvements were introduced
- Check that task boundaries were respected (no scope creep)

QUALITY REVIEW DIMENSIONS
--------------------------

### 1. ARCHITECTURE AND TECHNICAL STACK COMPLIANCE
Reference: `docs/contrib/review.md` â†’ Architecture Guidelines
- [ ] Only approved technologies from `decisions.md` are used
- [ ] Code is placed in appropriate architectural layers
- [ ] No architectural boundaries are violated
- [ ] Established patterns are followed consistently

### 2. CODE PRINCIPLES COMPLIANCE
Reference: `docs/contrib/review.md` â†’ Code Quality Validation
- [ ] SOLID principles are properly applied
  - [ ] Single Responsibility: classes have focused responsibilities
  - [ ] Open/Closed: extension without modification where appropriate
  - [ ] Dependency Inversion: proper abstraction and dependency injection
- [ ] DRY principle: no unnecessary code duplication
- [ ] KISS principle: solutions are appropriately simple
- [ ] Explicit failure: proper error handling, no silent fallbacks

### 3. TESTING QUALITY ASSURANCE
Reference: `docs/contrib/review.md` â†’ Testing Quality Assurance
- [ ] NO global mocking contamination (no `sys.modules` modifications without cleanup)
- [ ] Test isolation verified: tests pass individually AND as suite
- [ ] No shared mutable state between tests
- [ ] Proper cleanup patterns: tearDown/tearDownClass where needed
- [ ] Implementation-first approach: tests written after implementation
- [ ] Tests focus on behavior, not implementation details
- [ ] External dependencies properly mocked
- [ ] All important code paths covered
- [ ] Edge cases and error conditions tested
- [ ] Tests well-organized and logically structured

### 4. CODE STYLE AND CONSISTENCY
Reference: `docs/contrib/review.md` â†’ Code Style Quality Assurance
- [ ] Naming conventions followed consistently
- [ ] Type hints used appropriately for public APIs
- [ ] F-strings used for string formatting (no .format() or %)
- [ ] Imports organized logically (stdlib, third-party, local)
- [ ] Code style matches existing patterns

### 5. DOCUMENTATION QUALITY
Reference: `docs/contrib/review.md` â†’ Documentation Quality Assurance
- [ ] All public APIs have proper docstrings
- [ ] Single-line Google-style docstrings (no ending punctuation)
- [ ] Comments explain "why" rather than "what"
- [ ] Documentation consistent with project standards
- [ ] Complex logic properly documented

### 6. FOCUS AND SCOPE DISCIPLINE
Reference: `docs/contrib/review.md` â†’ Focus and Scope Validation
- [ ] Changes confined to requested scope only
- [ ] No unrelated changes introduced
- [ ] Existing interfaces preserved (backward compatibility)
- [ ] Code style matches existing patterns
- [ ] No "nice-to-have" features added without explicit request

REVIEW WORKFLOW (execution steps)
----------------------------------

**STEP 1: GATHER CONTEXT**
1. Identify the task being reviewed (from TASK_ID or git history)
2. Read the roadmap task section in `docs/technical/roadmap.md`
3. Review `decisions.md` to understand IN/OUT of scope boundaries
4. Get all staged changes: `git diff --cached --stat`
5. Get detailed diff: `git diff --cached`
6. **CRITICAL: Read EVERY staged file in FULL** - Use `read_file` to read the entire content (from line 1 to end) of **each and every modified AND added file** in staging
   - **MANDATORY FOR ALL STAGED FILES**: This applies equally to both newly created files AND existing files that were modified
   - For text files (.md, .py, .txt, etc.): Read from line 1 to EOF in one call
   - Never read in small chunks unless files exceed 1000+ lines
   - **DIFF-ONLY REVIEWS ARE INSUFFICIENT** - Full file content is required for proper code review and quality assessment
   - Exception: Binary files or files >2000 lines may be reviewed via diff + targeted reading

**STEP 2: SCOPE COMPLIANCE CHECK**
6. Compare staged changes against roadmap task checklist
7. Verify each modified/added file relates to task deliverables
8. Flag any out-of-scope changes or additions
9. Check for scope creep: unrequested features, refactoring, or improvements

**STEP 3: ARCHITECTURE REVIEW**
10. Verify technical stack compliance (Python, Django, Poetry, etc.)
11. Check architectural layer placement (models, services, API)
12. Verify no architectural boundaries violated
13. Confirm established patterns followed

**STEP 4: CODE QUALITY REVIEW**
**CRITICAL: Compare every code file against `docs/contrib/code.md` standards line-by-line**
14. Review SOLID principles application (reference: code.md â†’ SOLID Principles)
    - Single Responsibility: Each class/function does one thing
    - Open/Closed: Extension without modification where appropriate
    - Dependency Inversion: Proper abstractions and dependency injection
15. Check for DRY violations (reference: code.md â†’ Apply DRY)
    - No unnecessary code duplication
    - Reusable functions extracted appropriately
16. Verify KISS compliance (reference: code.md â†’ Apply KISS)
    - Solutions are appropriately simple
    - No overengineering beyond requirements
17. Review error handling (reference: code.md â†’ Apply Explicit Failure)
    - Proper exception handling with clear messages
    - No silent failures or bare except clauses
18. Check naming conventions and type hints (reference: code.md â†’ Clear Naming Conventions & Type Hints)
    - Descriptive, consistent names
    - Type hints on public APIs
19. Verify f-string usage and import organization (reference: code.md â†’ Modern F-Strings & Structure Imports)
    - F-strings used (no .format() or %)
    - Imports grouped: stdlib, third-party, local
20. Verify docstrings (reference: code.md â†’ Write Clear Docstrings)
    - Single-line Google-style format
    - No ending punctuation
    - Describes functionality, not arguments

**STEP 5: TESTING REVIEW**
20. Verify test files exist for new functionality
21. Check for global mocking contamination
22. Verify test isolation (run tests individually and as suite)
23. Review test organization and structure
24. Check behavior-focused testing (not implementation details)
25. Verify mock usage for external dependencies
26. Check test coverage of edge cases and error conditions

**STEP 6: DOCUMENTATION REVIEW**
27. Verify all public functions/classes have docstrings
28. Check docstring format (single-line Google-style)
29. Review comments for "why" vs "what" explanations
30. Verify documentation completeness

**STEP 7: VALIDATION**
31. Run linting: `./scripts/lint.sh`
32. Run unit and integration tests: `./scripts/test_unit.sh`
33. Run task-specific validation commands (from roadmap)
34. Verify all validation passes

**STEP 8: REVIEW REPORT**
35. Generate comprehensive review report following `docs/technical/release_review_template.md`
36. Save report as `docs/releases/release-v[VERSION]-review.md`
37. List all findings: PASS, ISSUES, BLOCKERS with specific file:line references
38. Provide specific, actionable recommendations for any issues
39. Include detailed quality metrics tables
40. Document competitive benchmark assessment if applicable
41. **Include "End-User Validation" section** with commands/steps for developers to manually test
42. Provide clear approval status and next steps

VALIDATION COMMANDS (copy from roadmap)
----------------------------------------
Run these to verify the implementation:
```bash
[VALIDATION_COMMANDS]
```

Required validation:
```bash
# Code quality
./scripts/lint.sh

# All tests
./scripts/test-all.sh

# Git status
git status
git diff --cached --stat
```

REVIEW REPORT FORMAT
---------------------
Generate a comprehensive structured review report following the template in `docs/technical/release_review_template.md`.

**CRITICAL FILENAME FORMAT**: Save the review report as `docs/releases/release-v[VERSION]-review.md`
- Example: `release-v0.53.3-review.md`
- Example: `release-v0.54.0-review.md`

This naming convention distinguishes review reports from implementation reports:
- Implementation: `release-v[VERSION]-implementation.md` (created by roadmap-task-implementation.prompt.md)
- Review: `release-v[VERSION]-review.md` (created by roadmap-task-review.prompt.md)

**Key sections to include:**
1. Executive Summary with overall status and key achievements
2. Scope Compliance Check (deliverables vs roadmap)
3. Architecture & Technical Stack Compliance
4. Code Quality Validation (SOLID, DRY, KISS)
5. Testing Quality Assurance
6. Component-Specific Content Quality (if applicable)
7. Documentation Quality
8. Validation Results (test/lint output)
9. **End-User Validation** (commands/steps for developer manual testing)
10. Findings Summary (Pass/Issues/Blockers)
11. Detailed Quality Metrics (tables)
12. Recommendations (strengths, required changes, future considerations)
13. Conclusion with approval status

**Template structure:**

```markdown
# Review Report: [TASK_ID] - [TASK_NAME]

**Task**: [SHORT_GOAL_STATEMENT]
**Release**: [RELEASE_VERSION]
**Review Date**: [DATE]
**Status**: [âœ… APPROVED / âš ï¸ APPROVED WITH ISSUES / âŒ REJECTED / ðŸš« BLOCKED]

## EXECUTIVE SUMMARY
Brief overview of the review findings (2-3 sentences).

## Scope Compliance
âœ… PASS / âŒ FAIL / âš ï¸  ISSUES

### Changes Reviewed
- List all modified/added/deleted files from `git diff --cached --stat`

### Scope Verification
- [ ] All changes relate to task deliverables
- [ ] No out-of-scope features added
- [ ] No unrelated refactoring introduced
- [ ] Task boundaries respected

**Findings**: [List any scope violations or concerns]

## Architecture Review
âœ… PASS / âŒ FAIL / âš ï¸  ISSUES

- [ ] Technical stack compliance
- [ ] Architectural layer placement
- [ ] Boundary adherence
- [ ] Pattern consistency

**Findings**: [List any architecture issues]

## Code Quality Review
âœ… PASS / âŒ FAIL / âš ï¸  ISSUES

### SOLID Principles
- [ ] Single Responsibility Principle
- [ ] Open/Closed Principle
- [ ] Dependency Inversion Principle

### Code Principles
- [ ] DRY (Don't Repeat Yourself)
- [ ] KISS (Keep It Simple)
- [ ] Explicit Failure

### Code Style
- [ ] Naming conventions
- [ ] Type hints
- [ ] F-strings
- [ ] Import organization

**Findings**: [List any code quality issues]

## Testing Review
âœ… PASS / âŒ FAIL / âš ï¸  ISSUES

- [ ] Tests exist for new functionality
- [ ] No global mocking contamination
- [ ] Test isolation verified
- [ ] Behavior-focused testing
- [ ] Proper mock usage
- [ ] Edge case coverage

**Findings**: [List any testing issues]

## Documentation Review
âœ… PASS / âŒ FAIL / âš ï¸  ISSUES

- [ ] Public APIs documented
- [ ] Docstring format compliance
- [ ] Comments explain "why"
- [ ] Documentation complete

**Findings**: [List any documentation issues]

## Validation Results
âœ… PASS / âŒ FAIL / âš ï¸  ISSUES

### Linting
```bash
$ ./scripts/lint.sh
[OUTPUT]
```

### Tests
```bash
$ ./scripts/test_unit.sh
[OUTPUT]
```

### Task-Specific Validation
```bash
[VALIDATION_COMMANDS]
[OUTPUT]
```

## End-User Validation
â¸ï¸  PENDING DEVELOPER TESTING

**Instructions for Developer**: After code review approval, please manually test this feature from an end-user perspective using the commands below. Update this section with your results.

### Manual Testing Steps

```bash
# Step 1: Clean environment setup
[Provide specific commands for this feature]

# Step 2: Installation verification
[Provide specific commands for this feature]

# Step 3: User journey testing
[Provide specific workflow commands for this feature]

# Step 4: Real-world scenarios
[Provide realistic test scenarios]

# Step 5: Output verification
[Commands to verify correct output]
```

### Validation Checklist
- [ ] Clean install tested in fresh environment
- [ ] Help text is clear and complete
- [ ] Error messages are user-friendly
- [ ] Happy path workflow succeeds
- [ ] Edge cases handled gracefully
- [ ] Documentation examples work when copy-pasted
- [ ] Performance is acceptable
- [ ] Output format matches expectations

**Developer**: Fill in results after manual testing and update status above to âœ… PASS / âŒ FAIL / âš ï¸ ISSUES

## Issues and Recommendations

### ðŸš¨ BLOCKERS (Must Fix Before Commit)
1. [Issue description with file:line reference]
   - Recommendation: [How to fix]

### âš ï¸  ISSUES (Should Fix)
1. [Issue description with file:line reference]
   - Recommendation: [How to fix]

### ï¿½ï¿½ SUGGESTIONS (Optional Improvements)
1. [Suggestion with rationale]

## Conclusion

**Overall Status**: [APPROVED FOR COMMIT / NEEDS REVISION / BLOCKED]

[Summary paragraph with overall assessment and next steps]

---
**Reviewed by**: AI Code Assistant
**Review Prompt**: roadmap-task-review.prompt.md
```

**IMPORTANT**: The above is a simplified example. For the complete, authoritative review report structure with all required sections, detailed quality metrics tables, competitive benchmarks, and usage guidance, refer to `docs/technical/release_review_template.md`.

The full template includes:
- Executive Summary with overall status (âœ…/âš ï¸/âŒ/ðŸš«) and key achievements
- Detailed scope compliance with roadmap checklist item-by-item comparison
- Architecture & technical stack verification tables
- Comprehensive code quality assessment (SOLID/DRY/KISS with examples)
- Testing quality assurance (contamination checks, isolation verification)
- Component-specific content quality sections (adapt based on task type)
- Documentation quality review (release docs, roadmap updates, docstrings)
- Validation results with actual command output
- Findings summary with Pass/Issues/Blockers breakdown
- Detailed quality metrics tables (test coverage, code quality scores, competitive benchmarks)
- Recommendations section (strengths, required changes, future considerations)
- Conclusion with clear approval status and next steps

REVIEW COMPLETION CRITERIA
---------------------------
The review is complete when:
- [ ] All review dimensions have been checked
- [ ] All validation commands have been run
- [ ] Review report has been generated with all sections filled
- [ ] Clear PASS/FAIL/ISSUES status determined for each dimension
- [ ] Specific recommendations provided for any issues found
- [ ] Overall approval status determined (APPROVED / NEEDS REVISION / BLOCKED)

AUTOMATIC TASK DETECTION
-------------------------
When `TASK_ID` is not provided, detect the reviewed task as follows:

1. Check recent git commit messages for task references (e.g., "Task 0.53.2", "v0.53.2")
2. Analyze staged file changes to infer which roadmap task they relate to
3. Scan `docs/technical/roadmap.md` for the most recently completed or in-progress task
4. Check for any release documentation in `docs/releases/` that matches staged changes
5. Record the detected Task ID into the TASK SUMMARY section before proceeding

If task cannot be detected automatically:
- Analyze staged changes and provide best-guess task identification
- Note in review report that task was inferred rather than explicitly specified

CRITICAL REVIEW PRINCIPLES
---------------------------

### Read ALL Staged Files in FULL (MANDATORY)
- **NEVER review code based solely on diffs** - diffs show changes but not full context
- Use `read_file` to read every staged file completely (line 1 to end)
- For Python files: Read entire file to understand class/function context
- For documentation: Read full content to verify completeness
- Small chunk reading is ONLY acceptable for files exceeding 1000 lines
- This is non-negotiable for thorough review

### Compare Every Code File Against code.md Standards (MANDATORY)
- Open `docs/contrib/code.md` and use it as your review checklist
- For each Python file, verify line-by-line compliance with:
  - SOLID principles (SRP, OCP, DIP examples in code.md)
  - DRY principle (no duplication)
  - KISS principle (appropriate simplicity)
  - Explicit failure (proper error handling)
  - Naming conventions and type hints
  - F-string usage
  - Import organization
  - Docstring format (single-line Google-style)
- Reference specific sections of code.md in your findings

### Scope Discipline is Paramount
- The #1 priority is verifying scope compliance
- ANY out-of-scope change is a violation requiring explanation
- Features not in the roadmap task = automatic review failure

### No Rubber Stamping
- Provide specific, actionable feedback
- Reference exact file:line locations for issues
- Give concrete recommendations for fixes

### Standards Over Preferences
- Review against documented standards, not personal preferences
- All standards are in the authoritative context files
- If it's not in the standards docs, it's not a valid issue

### Fail Fast on Blockers
- Clearly distinguish BLOCKERS from ISSUES from SUGGESTIONS
- Blockers must be fixed before commit
- Be explicit about what prevents approval

END-USER VALIDATION (FOR REVIEW REPORT)
----------------------------------------

### Purpose
This section provides **validation commands and steps to include in your review report** for the human developer to execute manually. After technical validation (linting, tests, code review), developers should verify their changes work correctly from an end-user perspective.

**IMPORTANT FOR AI REVIEWERS**:
- **DO NOT execute these commands yourself**
- **DO include these commands/steps in the review report** you generate
- Adapt the commands to the specific feature being reviewed
- Place them in an "End-User Validation" section of the review report
- These commands are for the developer to run manually after reading your review

### When Developers Should Perform End-User Validation
- **Always** for user-facing features (CLI commands, API endpoints, UI changes)
- **Always** for changes affecting installation, deployment, or configuration
- **Always** for features that modify user workflows or data
- Optional for internal refactoring with no user-visible changes

---

## Validation Commands Reference (Include in Review Report)

The following sections provide example commands for developers to manually test their implementations. Adapt these to the specific feature and include relevant commands in your review report's "End-User Validation" section.

### Step 1: Clean Environment Setup
Developers should simulate a fresh user installation to catch environment-specific issues:

```bash
# Create a clean test environment (choose appropriate method)
# Option A: New virtual environment
python -m venv /tmp/qs-test-env
source /tmp/qs-test-env/bin/activate

# Option B: Clean Poetry environment
cd /tmp
mkdir quickscale-test && cd quickscale-test
poetry init --no-interaction
poetry add /path/to/quickscale/dist/quickscale_core-X.X.X-py3-none-any.whl
poetry add /path/to/quickscale/dist/quickscale_cli-X.X.X-py3-none-any.whl

# Option C: Docker container (for deployment testing)
docker run -it --rm python:3.11 bash
```

### Step 2: Installation Verification
Developers should test the installation process as a new user would:

```bash
# Verify package installation
pip install quickscale  # or appropriate install command
quickscale --version
quickscale --help

# Check for import errors
python -c "import quickscale_core; print('Core OK')"
python -c "from quickscale_cli import main; print('CLI OK')"

# Verify all dependencies installed correctly
pip list | grep quickscale
```

### Step 3: User Journey Testing
Developers should walk through the complete user workflow that the feature enables:

```bash
# Example for CLI feature:
# 1. Initialize/setup (if required)
quickscale init my-project
cd my-project

# 2. Use the new feature with typical inputs
quickscale [new-command] [typical-args]

# 3. Verify output/results
ls -la  # Check created files
cat output.txt  # Check generated content
quickscale status  # Check system state

# 4. Test common variations
quickscale [new-command] --flag
quickscale [new-command] --alternative-mode

# 5. Verify integration with existing features
quickscale existing-command
quickscale [new-command] | quickscale other-command  # Pipeline testing
```

### Step 4: Real-World Scenarios
Developers should test with realistic data and use cases:

```bash
# Use actual project structures, not minimal test fixtures
cd ~/projects/real-django-app
quickscale [new-command]

# Test with edge cases users might encounter
quickscale [new-command] --large-dataset data/10k-records.json
quickscale [new-command] --special-chars "name with spaces & symbols"
quickscale [new-command] --empty-input ""

# Test error handling with invalid inputs
quickscale [new-command] --invalid-flag
quickscale [new-command] missing-required-arg
```

### Step 5: Output Verification
Developers should ensure user-facing output is correct and helpful:

```bash
# Check command output formatting
quickscale [new-command] | less  # Verify readability
quickscale [new-command] --json | jq  # Verify structured output
quickscale [new-command] --verbose  # Verify debug info

# Verify error messages are clear
quickscale [new-command] --bad-input 2>&1 | cat
# Should see: clear error message, not a stack trace

# Check help text
quickscale [new-command] --help
# Should see: clear description, all options documented
```

### Step 6: Persistence and State
For features that modify files or state, developers should verify:

```bash
# Verify file creation/modification
ls -la expected/output/path
cat generated-file.txt
git diff  # If modifying version-controlled files

# Verify configuration changes persist
quickscale config set key value
quickscale config get key  # Should return "value"
# Restart and verify
quickscale config get key  # Should still return "value"

# Verify database migrations (if applicable)
python manage.py showmigrations
python manage.py migrate --fake-initial
```

### Step 7: Integration Validation
Developers should test interaction with other components:

```bash
# If CLI feature, test with CI/CD pipeline
bash -c "quickscale [new-command] && echo 'Success'" || echo 'Failed'

# If API feature, test with curl/httpie
curl -X POST http://localhost:8000/api/new-endpoint \
  -H "Content-Type: application/json" \
  -d '{"test": "data"}'

# If module feature, test programmatic usage
python <<EOF
from quickscale_core import NewFeature
result = NewFeature().do_something()
assert result.success, "Feature failed"
print("Integration test passed")
EOF
```

### Step 8: Documentation Walkthrough
Developers should follow the documentation as a new user would:

```bash
# Open the relevant docs
cat README.md  # or docs/features/new-feature.md

# Execute each example command from docs exactly as written
# (Copy-paste from docs, don't type manually)
[command-from-docs-line-45]
[command-from-docs-line-52]

# Verify example outputs match documentation
# If docs say "This will create a config.yml file", verify:
[ -f config.yml ] && echo "âœ“ Documented behavior confirmed"
```

### Step 9: Performance and Usability
Developers should check that the feature performs acceptably in real use:

```bash
# Measure execution time for typical operations
time quickscale [new-command] typical-input.txt
# Should complete in reasonable time (document if >5s)

# Test with realistic data volumes
quickscale [new-command] large-project/
# Should not hang or crash

# Verify progress indicators for long operations
quickscale [new-command] --large-dataset
# Should show progress bar or status updates
```

### Step 10: Cleanup and Uninstallation
Developers should verify clean removal (if applicable):

```bash
# Test feature cleanup commands
quickscale [new-command] --cleanup
quickscale remove [feature]

# Verify uninstallation
pip uninstall quickscale -y
pip list | grep quickscale  # Should be empty

# Check for leftover files
ls ~/.quickscale  # Should be removed or minimal
```

### End-User Validation Checklist

Include this checklist in the review report:

- [ ] **Clean Install**: Feature works in fresh environment (not just dev environment)
- [ ] **Help Text**: All commands have clear `--help` output
- [ ] **Error Messages**: Failures show helpful messages, not stack traces
- [ ] **Happy Path**: Primary user workflow completes successfully
- [ ] **Edge Cases**: Handles empty inputs, special characters, large data
- [ ] **Error Recovery**: Graceful handling of invalid inputs/failures
- [ ] **Documentation**: All examples in docs actually work when copy-pasted
- [ ] **Performance**: Completes in reasonable time with realistic data
- [ ] **Output Format**: Generated files/output match expected structure
- [ ] **Persistence**: State/config changes survive restart
- [ ] **Integration**: Works with existing features/workflows
- [ ] **Cleanup**: Uninstall/remove leaves no orphaned files

### Common End-User Validation Commands

Adapt these templates to the specific feature being reviewed:

```bash
# === FOR CLI FEATURES ===
# Basic functionality
quickscale [command] --help
quickscale [command] [args]
quickscale [command] --version

# Error handling
quickscale [command]  # Missing required args
quickscale [command] --invalid-flag
quickscale [command] nonexistent-file.txt

# === FOR API FEATURES ===
# Start test server
python manage.py runserver &
SERVER_PID=$!

# Test endpoints
curl -X GET http://localhost:8000/api/health
curl -X POST http://localhost:8000/api/[endpoint] \
  -H "Content-Type: application/json" \
  -d @test-payload.json

# Cleanup
kill $SERVER_PID

# === FOR MODULE FEATURES ===
# Interactive Python testing
python3 <<EOF
from quickscale_core.new_module import NewClass
obj = NewClass()
result = obj.main_method()
print(f"Result: {result}")
assert result is not None
EOF

# === FOR SCAFFOLDING/GENERATORS ===
# Generate and verify
quickscale generate [type] test-output
cd test-output
./scripts/test-all.sh  # Verify generated project works
cd .. && rm -rf test-output

# === FOR DEPLOYMENT FEATURES ===
# Deploy to test environment
quickscale deploy test-env
# Verify deployment
curl https://test-env.example.com/health
# Rollback test
quickscale rollback test-env
```

### Template for End-User Validation Results

Include this template in the review report for developers to fill out after completing manual validation:

```markdown
## End-User Validation
âœ… PASS / âŒ FAIL / âš ï¸  ISSUES

### Environment
- Installation method: [pip/poetry/docker]
- Python version: [version]
- OS: [linux/macos/windows]
- Test duration: [time spent testing]

### User Journeys Tested
1. **[Primary Workflow Name]**
   - Commands: `quickscale init`, `quickscale [new-command]`
   - Result: âœ… Success / âš ï¸ Minor issues / âŒ Failure
   - Notes: [observations]

2. **[Alternative Workflow Name]**
   - Commands: `quickscale [command] --flag`
   - Result: âœ… Success
   - Notes: [observations]

### Real-World Scenarios
- Tested with: [description of realistic data/project]
- Edge cases verified: [list]
- Performance: [time measurements]

### Issues Found
- [ ] **Issue 1**: [description]
  - Steps to reproduce: [commands]
  - Expected: [behavior]
  - Actual: [behavior]
  - Severity: [blocker/issue/suggestion]

### User Experience Observations
- Clear error messages: âœ… Yes / âŒ No
- Documentation accuracy: âœ… Yes / âŒ No
- Intuitive workflow: âœ… Yes / âš ï¸ Needs improvement
- Performance acceptable: âœ… Yes / âŒ No

**Overall UX Status**: [PASS / NEEDS IMPROVEMENT / FAIL]
```

### Why End-User Validation Matters

- **Tests â‰  Reality**: Unit tests verify code correctness, but don't catch UX issues
- **Environment Differences**: Dev environments have different configs than production
- **Documentation Verification**: Ensures examples actually work
- **Error Message Quality**: Automated tests rarely check error message clarity
- **Integration Issues**: Real workflows reveal integration problems tests miss
- **Performance Reality**: Tests use minimal data; real usage reveals bottlenecks

### End-User Validation is MANDATORY For:

1. **New CLI commands** - Must be tested in clean shell
2. **API endpoints** - Must be tested with curl/httpie
3. **Scaffolding/generators** - Must verify generated output works
4. **Installation changes** - Must test fresh install
5. **Configuration features** - Must verify persistence
6. **Documentation updates** - Must verify all examples work

### AI Reviewer Responsibilities

When generating the review report:
1. **Include an "End-User Validation" section** with appropriate validation commands adapted to the specific feature
2. **Check if the developer has documented their manual testing** (look for evidence in implementation reports or commit messages)
3. **Remind developers** to complete end-user validation before considering the review complete
4. **Provide the validation template** for developers to fill out after manual testing

The review report should guide developers to perform end-user validation themselves, not assume it has been done.

START REVIEW NOW
----------------
Begin by:
1. Detecting or confirming the TASK_ID
2. Loading the roadmap task section from `docs/technical/roadmap.md`
3. Getting all staged changes with `git diff --cached --stat`
4. **Reading ALL staged files in FULL** using `read_file` (entire file, line 1 to end)
5. Reading all authoritative context files (especially `docs/contrib/code.md` for code standards)
6. Reading the review report template from `docs/technical/release_review_template.md`
7. Comparing each code file against the specific rules and examples in `docs/contrib/code.md`
8. Executing the review workflow step-by-step
9. Generating the comprehensive review report following the template structure

**MANDATORY PRE-REVIEW CHECKLIST:**
- [ ] Identified all staged files from `git diff --cached --stat`
- [ ] Read EVERY staged file in FULL using `read_file` (not just diffs)
- [ ] Read `docs/contrib/code.md` completely to use as review reference
- [ ] Read `docs/contrib/review.md` for quality checklist
- [ ] Read roadmap task section to understand scope

Remember: Your role is to ensure quality and scope compliance, not to approve everything. Be thorough, specific, and standards-based in your review. **You cannot properly review code without reading the full files.**
